---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: active-health-checks-rvs-low
value: -100000
globalDefault: false
preemptionPolicy: PreemptLowerPriority
description: "Very low priority for active health check jobs to be preempted by others"
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: active-health-checks-runner
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: active-health-checks-runner-role
rules:
  - apiGroups: ["batch"]
    resources: ["jobs"]
    verbs: ["create", "get", "list", "watch", "update", "patch", "delete"]
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["pods/log"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch", "patch", "update"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["get", "list", "watch", "create", "update"]
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create", "get", "list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: active-health-checks-runner-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: active-health-checks-runner-role
subjects:
  - kind: ServiceAccount
    name: active-health-checks-runner
    namespace: monitoring
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: active-health-checks-rvs-applier
  namespace: monitoring
spec:
  schedule: "0 * * * *"
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 300
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 0
  jobTemplate:
    spec:
      backoffLimit: 1
      template:
        spec:
          serviceAccountName: active-health-checks-runner
          restartPolicy: OnFailure
          containers:
          - name: applier
            image: iad.ocir.io/idxzjcdglx2s/kubectl:latest
            command: ["/bin/sh", "-c"]
            args:
            - |
              set -euo pipefail
              JOB_NAME="active-health-checks-rvs-$(date +%s)"
              echo "[applier] Creating RVS (ROCm Validation Suite) Job: $JOB_NAME"
              
              # Check which nodes were tested recently and should be excluded
              current_date=$(date -u +%Y-%m-%d)
              excluded_nodes=""
              available_nodes=""
              idle_candidates=""
              gpu_nodes=$(kubectl get nodes -l amd.com/gpu=true -o jsonpath='{range .items[*]}{.metadata.name}{" "}{end}' | tr ' ' '\n' | grep -v '^$')
              
              for node in $gpu_nodes; do
                gpu_usage=$(kubectl get pods -A --field-selector spec.nodeName=$node -o jsonpath='{range .items[*]}{range .spec.containers[*]}{.resources.requests.amd\\.com/gpu}{"\n"}{end}{end}' 2>/dev/null | awk 'NF{sum+=$1+0} END{print sum+0}')
                if [ "$gpu_usage" -eq 0 ] 2>/dev/null; then
                  echo "[applier] Node $node is idle (allocated GPU usage = 0)"
                  idle_candidates="$idle_candidates $node"
                else
                  echo "[applier] Node $node currently uses $gpu_usage GPU(s) - skipping"
                fi
              done
              
              for node in $idle_candidates; do
                last_run_label=$(kubectl get node "$node" -o jsonpath='{.metadata.labels.oke\.oraclecloud\.com/active-health-checks-rvs-last-run}' 2>/dev/null || echo "")
                if [ -n "$last_run_label" ]; then
                  last_run_date=$(echo "$last_run_label" | cut -d'T' -f1)
                  if [ "$last_run_date" = "$current_date" ]; then
                    echo "[applier] Excluding node $node from scheduling - tested today"
                    excluded_nodes="$excluded_nodes $node"
                  else
                    echo "[applier] Node $node available for testing - last tested on $last_run_date"
                    available_nodes="$available_nodes $node"
                  fi
                else
                  echo "[applier] Node $node available for testing - no previous test timestamp found"
                  available_nodes="$available_nodes $node"
                fi
              done
              
              # Count available nodes
              available_count=$(echo "$available_nodes" | tr ' ' '\n' | grep -v '^$' | wc -l)
              echo "[applier] Found $available_count available nodes for testing"
              if [ $available_count -eq 0 ]; then
                echo "[applier] No idle nodes available; skipping job creation"
                exit 0
              fi
              
              # For RVS, we test one node at a time
              test_node=$(echo "$available_nodes" | tr ' ' '\n' | grep -v '^$' | head -n1)
              echo "[applier] Selected node $test_node for RVS test"
              
              # Get GPU count for the node
              gpu_count=$(kubectl get node "$test_node" -o jsonpath='{.status.capacity.amd\.com/gpu}' 2>/dev/null || echo "8")
              echo "[applier] Node $test_node has $gpu_count GPUs"
              
              # Create ConfigMap for RVS test configuration
              cat <<EOF | kubectl apply -f -
              apiVersion: v1
              kind: ConfigMap
              metadata:
                name: rvs-config-$JOB_NAME
                namespace: monitoring
              data:
                config.json: |
                  {
                    "TestConfig": {
                      "GPU_HEALTH_CHECK": {
                        "TestLocationTrigger": {
                          "global": {
                            "TestParameters": {
                              "MANUAL": {
                                "TestCases": [
                                  {
                                    "Recipe": "gst_single",
                                    "Iterations": 1,
                                    "StopOnFailure": true,
                                    "TimeoutSeconds": 600,
                                    "Arguments": "--parallel"
                                  }
                                ]
                              }
                            }
                          }
                        }
                      }
                    }
                  }
              EOF
              
              # Create Kubernetes Job for RVS
              cat <<EOF | kubectl apply -f -
              apiVersion: batch/v1
              kind: Job
              metadata:
                name: $JOB_NAME
                namespace: monitoring
              spec:
                backoffLimit: 0
                ttlSecondsAfterFinished: 300
                template:
                  metadata:
                    labels:
                      app: rvs-test
                  spec:
                    priorityClassName: active-health-checks-rvs-low
                    restartPolicy: Never
                    nodeSelector:
                      kubernetes.io/hostname: $test_node
                    volumes:
                    - name: config-volume
                      configMap:
                        name: rvs-config-$JOB_NAME
                    - name: test-runner-volume
                      hostPath:
                        path: /var/log/amd-test-runner
                        type: DirectoryOrCreate
                    containers:
                    - name: amd-test-runner
                      image: docker.io/rocm/test-runner:v1.4.0
                      imagePullPolicy: IfNotPresent
                      resources:
                        limits:
                          amd.com/gpu: $gpu_count
                        requests:
                          amd.com/gpu: $gpu_count
                      securityContext:
                        privileged: true
                      volumeMounts:
                      - mountPath: /var/log/amd-test-runner
                        name: test-runner-volume
                      - mountPath: /etc/test-runner/
                        name: config-volume
                      env:
                      - name: TEST_TRIGGER
                        value: "MANUAL"
                      - name: POD_NAME
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.name
                      - name: POD_NAMESPACE
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.namespace
                      - name: NODE_NAME
                        valueFrom:
                          fieldRef:
                            fieldPath: spec.nodeName
              EOF
              
              echo "[applier] Waiting for Job $JOB_NAME to complete..."
              
              # Wait for job to complete
              result="fail"
              for i in $(seq 1 180); do # up to 30 minutes
                job_status=$(kubectl get job -n monitoring "$JOB_NAME" -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
                job_failed=$(kubectl get job -n monitoring "$JOB_NAME" -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null || echo "")
                
                if [ "$job_status" = "True" ]; then
                  echo "[applier] Job completed successfully"
                  # Get the pod logs to check for successful completion
                  pod_name=$(kubectl get pods -n monitoring -l job-name="$JOB_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
                  if [ -n "$pod_name" ]; then
                    echo "[applier] Checking logs from pod $pod_name"
                    logs=$(kubectl logs -n monitoring "$pod_name" 2>/dev/null || echo "")
                    # Check for pattern "cmd gst_single [iteration=1, pid=*] completed successfully"
                    if echo "$logs" | grep -qE "cmd gst_single \[iteration=1, pid=[0-9]+\] completed successfully"; then
                      echo "[applier] RVS test PASSED - gst_single completed successfully"
                      result="pass"
                    else
                      echo "[applier] RVS test FAILED - 'cmd gst_single completed successfully' not found in logs"
                      result="fail"
                    fi
                  else
                    echo "[applier] Could not find pod for job"
                    result="fail"
                  fi
                  break
                elif [ "$job_failed" = "True" ]; then
                  echo "[applier] Job failed"
                  result="fail"
                  break
                fi
                
                sleep 10
              done
              
              echo "[applier] Result for $JOB_NAME: $result"
              echo "[applier] Test node: $test_node"
              
              # Label the node with test results
              timestamp=$(date -u +%Y-%m-%dT%H-%M-%SZ)
              echo "[applier] Labeling node $test_node with oke.oraclecloud.com/active-health-checks-rvs=$result"
              kubectl label node "$test_node" "oke.oraclecloud.com/active-health-checks-rvs=$result" --overwrite
              echo "[applier] Labeling node $test_node with oke.oraclecloud.com/active-health-checks-rvs-last-run=$timestamp"
              kubectl label node "$test_node" "oke.oraclecloud.com/active-health-checks-rvs-last-run=$timestamp" --overwrite
              
              # Clean up the ConfigMap
              echo "[applier] Cleaning up ConfigMap rvs-config-$JOB_NAME"
              kubectl delete configmap -n monitoring "rvs-config-$JOB_NAME" --ignore-not-found=true
              
              if [ "$result" = "fail" ]; then
                echo "[applier] RVS test failed but node has been labeled"
                exit 0
              fi
              
              echo "[applier] RVS test completed successfully"


