apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "oke-ons-webhook.fullname" . }}
  labels:
    {{- include "oke-ons-webhook.labels" . | nindent 4 }}
data:
  notification_template.j2: |
    {{ .Files.Get "files/notification_template.j2" | nindent 4 }}
  script.py: |
    # /// script
    # requires-python = ">=3.12"
    # dependencies = [
    #   "oci",
    #   "requests",
    #   "uvicorn",
    #   "fastapi",
    #   "kubernetes",
    #   "jinja2"
    # ]
    # ///

    import base64
    import hashlib
    import json
    import logging
    import os
    import queue
    import re
    import sqlite3
    import sys
    import traceback

    from time import sleep
    from datetime import datetime, timedelta, time
    from pathlib import Path
    from threading import Thread

    import oci
    import requests
    from urllib.parse import urlparse

    import uvicorn
    from fastapi import FastAPI, Request
    from fastapi import status as fastapi_status
    from fastapi.responses import JSONResponse
    from jinja2 import Template
    from kubernetes import client, config

    app = FastAPI()

    topic_id = os.environ.get("ONS_TOPIC_OCID")
    template_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), "notification_template.j2")
    db_dir = "/tmp"
    db_name = 'grafana_alert_processing_daemon.sqlite'

    grafana_service_url = os.environ.get("GRAFANA_SERVICE_URL")
    grafana_api_key = os.environ.get("grafana_api_key", "")

    minimum_age_of_active_alert_to_include_in_reminder_in_seconds = int(os.environ.get("MIN_AGE_OF_ACTIVE_ALERT_TO_INCLUDE_IN_REMINDER_IN_SECONDS", "43200")) #12 hours
    maximum_age_of_alert_in_seconds = int(os.environ.get("MAX_AGE_OF_ALERT_IN_SECONDS", "259200")) #3 days
    active_alerts_reminder_time = time(11, 0)
    push_to_oci_topic_interval_seconds = int(os.environ.get("PUSH_TO_OCI_TOPIC_INTERVAL_SECONDS", "30"))
    aggregate_notifications = True
    max_notification_size = 64000


    alert_queue = queue.Queue(maxsize=1000)
    notification_queue = queue.Queue(maxsize=1000)

    def base64_decode(s):
        return base64.b64decode(s).decode('utf-8')

    def utf8len(s):
        return len(s.encode('utf-8'))

    def str_to_date(text_date):
        clean_date_match = re.match(r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}", text_date)
        if clean_date_match:
            clean_date = clean_date_match.group(0)
            return datetime.strptime(f'{clean_date}Z', '%Y-%m-%dT%H:%M:%SZ')


    def date_to_str(date):
        return date.strftime('%Y-%m-%dT%H:%M:%SZ')


    def push_notifications_to_oci_topic(notification_client, notifications):
        try:
            alert_message = "\n".join([template.render(**notification) for notification in notifications])
            
            message_details = oci.ons.models.MessageDetails(
                title="GPU Cluster Alert",
                body=alert_message
            )
            
            response = notification_client.publish_message(
                topic_id=topic_id,  # Use the dynamically fetched topic_id
                message_details=message_details,
                retry_strategy=oci.retry.DEFAULT_RETRY_STRATEGY
            )

            logging.info(f"Message published. Message ID: {response.data.message_id}")
        except Exception as e:
            logging.error(f"Error publishing message: {traceback.format_exc()}")


    def shorten_url(grafana_service_url, grafana_api_key, url):
        # Shorten URL using Grafana API
        try:
            parsed_url = urlparse(url)
            
            path = parsed_url.geturl().replace(f'{parsed_url.scheme}://{parsed_url.netloc}/', "")
            headers = {
                "Accept": "application/json",
                "Content-Type": "application/json",
                "Authorization": f"Bearer {grafana_api_key}"
            }

            payload = {
                "path": path
            }

            response = requests.post(f"{grafana_service_url.rstrip('/')}/api/short-urls", json=payload, headers=headers)
            logging.debug(f'Attempt to shorten {path} returned code {response.status_code} and response: {response.json()}')
            
            if response.status_code == 200:
                short_url = response.json().get("url") 
                parsed_short_url = urlparse(short_url)
                netloc = parsed_url.netloc.split(":")[0]
                ip_only = False
                match = re.match(r"^(\d{1,3}\.?){4}$", netloc)
                if match:
                    ip_only = True
                if not ip_only:
                    scheme = "https"
                    return parsed_short_url.geturl().replace(f'{parsed_short_url.scheme}://{parsed_url.netloc}/', f"{scheme}://{netloc}/")
                return short_url
            else:
                return url
        except Exception as e:
            logging.error(f"Failed attempting to shorten URL: {traceback.format_exc()}")
            return url


    def alert_to_notification_dict(alert_id, alert_dict, fire_count, first_seen_at, last_seen_at):
        annotations = alert_dict.get("annotations", {})
        labels = alert_dict.get("labels", {})
        silence_url = annotations.get("silence_url", alert_dict.get("silenceURL", "N/A")).strip()
        panel_url =  annotations.get("panel_url", alert_dict.get("panelURL", "N/A")).strip()
        if silence_url and silence_url != "N/A":
            shorten_silence_url = shorten_url(grafana_service_url, grafana_api_key, silence_url)
        else:
            shorten_silence_url = "N/A"
        if panel_url and panel_url != "N/A":
            shorten_panel_url = shorten_url(grafana_service_url, grafana_api_key, panel_url)
        else:
            shorten_panel_url = "N/A"
        return {
            "alert_id": alert_id,
            "alert_name": labels.get("rulename", labels.get("alertname", "N/A")),
            "alert_status": alert_dict.get("status", "N/A"),
            "starts_at": alert_dict.get("startsAt", "N/A"),
            "ends_at": alert_dict.get("endsAt", "N/A") if alert_dict.get("endsAt", "N/A") != '0001-01-01T00:00:00Z' else "-",
            "description": annotations.get("summary", "N/A"),
            "hostname": labels.get("hostname", labels.get("Hostname", "N/A")),
            "node": labels.get("node", "N/A"),
            "oci_name": labels.get("oci_name", "N/A"),
            "serial_number": labels.get("host_serial_number", labels.get("serial", "N/A")),
            "interface": labels.get("interface", "N/A"),
            "rdma_device": labels.get("rdma_device", "N/A"),
            "gpu": labels.get("gpu", "N/A"),
            "fire_count": fire_count,
            "first_seen_at": first_seen_at,
            "last_seen_at": last_seen_at,
            "silence_url": shorten_silence_url,
            "panel_url": shorten_panel_url 
        }


    def notification_processing_daemon():
        try:
            # Use Instance Principals for authentication
            signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
            notification_client = oci.ons.NotificationDataPlaneClient({}, signer=signer)
        except Exception as e:
            logging.error(f"Error initializing OCI client: {e}")
            sys.exit(1)

        notifications = []
        seen_alert_ids = set()  # Track seen alerts in current batch to prevent duplicates

        while True:

            if aggregate_notifications:
                # Get all the notifications from the queue
                try:
                    notification_from_queue = notification_queue.get(block=True, timeout=push_to_oci_topic_interval_seconds)

                    # Skip if we've already seen this alert in the current batch
                    alert_id = notification_from_queue.get("alert_id")
                    if alert_id in seen_alert_ids:
                        logging.debug(f"Skipping duplicate alert {alert_id} in current batch")
                        continue

                    # Check if the aggregated notification text is larger than max_notification_size (ONS supports maximum 64 KB)
                    if utf8len("\n".join([template.render(**notification) for notification in (notifications + [notification_from_queue])])) > max_notification_size:
                        push_notifications_to_oci_topic(notification_client, notifications)
                        notifications = [notification_from_queue]
                        seen_alert_ids = set()
                    else:
                        notifications.append(notification_from_queue)
                    seen_alert_ids.add(alert_id)

                    # wait five seconds before checking if the queue is empty (avoid accumulating too much delay sending notifications when aggregate_notifications is True)
                    sleep(5)

                    if notification_queue.empty():
                        push_notifications_to_oci_topic(notification_client, notifications)
                        notifications = []
                        seen_alert_ids = set()

                # When queue is empty and there are notifications to be sent, send them
                except queue.Empty:
                    if notifications:
                        push_notifications_to_oci_topic(notification_client, notifications)
                    notifications = []
                    seen_alert_ids = set()
            else:
                try:
                    notification_from_queue = notification_queue.get(block=True, timeout=push_to_oci_topic_interval_seconds)
                    alert_id = notification_from_queue.get("alert_id")
                    if alert_id not in seen_alert_ids:
                        notifications.append(notification_from_queue)
                        seen_alert_ids.add(alert_id)
                except queue.Empty:
                    pass

                if notifications:
                    push_notifications_to_oci_topic(notification_client, notifications)
                notifications = []
                seen_alert_ids = set()


    def alert_processing_daemon():
        try:
            os.makedirs(db_dir, exist_ok=True)
            conn = sqlite3.connect(os.path.join(db_dir, db_name))
            cur = conn.cursor()
            # Create active_alerts table if it doesn't exist
            cur.execute('''CREATE TABLE IF NOT EXISTS active_alerts (
                alert_id TEXT PRIMARY KEY,
                first_seen_at TEXT NOT NULL,
                last_seen_at TEXT NOT NULL,
                last_notified_at TEXT NOT NULL,
                ended_at TEXT NOT NULL,
                fire_count INTEGER NOT NULL,
                payload TEXT NOT NULL
            )''')
            conn.commit()
            # Create alert_reminder table if it doesn't exist
            cur.execute('''CREATE TABLE IF NOT EXISTS alert_reminder (
                notification_id TEXT PRIMARY KEY,
                last_executed_at TEXT NOT NULL
            )''')
            conn.commit()

            last_reminder_execution_date_from_db = cur.execute('SELECT * FROM alert_reminder WHERE notification_id = ?', ("last",)).fetchone()
            if last_reminder_execution_date_from_db:
                last_reminder_execution_day = str_to_date(last_reminder_execution_date_from_db[1]).date()
            else:
                last_reminder_execution_day = None

        except Exception as e:
            logging.error(f'Failed to create the databases for alerts {e}. Exiting..')
            sys.exit(1)
        
        logging.info(f'Last reminder execution day: {last_reminder_execution_day}')
        
        while True:
            # Process alerts in the queue
            current_time = datetime.now()

            try:
                alert = alert_queue.get(block=True, timeout=30)

                # Parse the alert JSON
                alert_dict = json.loads(alert)
                
                # Skip alert with name "DatasourceNoData"
                alert_labels = alert_dict.get('labels', {})
                alert_name = alert_labels.get('alertname', alert_labels.get('rulename', 'None'))
                

                if alert_name and alert_name != "DatasourceNoData":

                    # Add alert details into the database
                    
                    starts_at = str_to_date(alert_dict.get('startsAt'))

                    # Generate alert_id - use fingerprint directly when available (it's already unique)
                    # Fallback uses hash of alert_name + labels for consistent deduplication
                    fingerprint = alert_dict.get("fingerprint", "")
                    if fingerprint:
                        alert_id = fingerprint
                    else:
                        labels_str = json.dumps(alert_labels, sort_keys=True)
                        alert_id = hashlib.sha256(f'{alert_name}-{labels_str}'.encode('utf-8')).hexdigest()
                    ends_at = str_to_date(alert_dict.get('endsAt'))
                    current_status = alert_dict.get('status', "Status Unknown")

                    # Check if the alert is present in the database
                    alert_in_db = cur.execute('SELECT * FROM active_alerts WHERE alert_id = ?', (alert_id,)).fetchone()
                    if alert_in_db:
                        # If the alert is resolved, clear the alert from database and send load the notification into the queue
                        if ends_at > starts_at and current_status != "firing":
                            notification_dict = alert_to_notification_dict(alert_in_db[0], alert_dict, alert_in_db[5], alert_in_db[1], date_to_str(ends_at))
                            notification_queue.put(notification_dict)
                            
                            logging.info(f"Alert {alert_id}/{alert_name} resolved. Notification sent.")
                            
                            cur.execute('''
                                DELETE FROM active_alerts
                                WHERE alert_id = ?
                            ''', (alert_id,))
                        else: 
                            # If the alert is not resolved, increment the fire_count
                            cur.execute('''
                                UPDATE active_alerts
                                SET last_seen_at = ?, fire_count = fire_count + 1, payload = ?
                                WHERE alert_id = ?
                            ''', (date_to_str(current_time), alert, alert_id))
                            logging.info(f"Alert {alert_id}/{alert_name} still active. Incremented Fire Count.")
                    else:
                        # If alert received for the first time and is already clear, just send notification immediately
                        if ends_at > starts_at and current_status != "firing":
                            notification_dict = alert_to_notification_dict(alert_id, alert_dict, 1, alert_dict.get('startsAt'), alert_dict.get('endsAt'))
                            notification_queue.put(notification_dict)
                            logging.info(f"New Alert {alert_id}/{alert_name} seen for the first time with status resolved. Notification sent.")

                        # otherwise, load the alert into the database but DEFER notification (10s debounce)
                        else:
                            cur.execute('''
                                INSERT INTO active_alerts (alert_id, first_seen_at, last_seen_at, last_notified_at, ended_at, fire_count, payload)
                                VALUES (?, ?, ?, ?, ?, ?, ?)
                            ''', (alert_id, date_to_str(current_time), date_to_str(current_time), "0001-01-01T00:00:00Z", "0001-01-01T00:00:00Z", 1, alert))
                            logging.info(f"New Alert {alert_id}/{alert_name} stored. Notification deferred for debounce.")

                    # Commit the changes to the database
                    conn.commit()
                else:
                    logging.debug(f'Skipping processing alert with name DatasourceNoData or empty: {alert}')
                    pass
            except queue.Empty:
                pass
            except Exception:
                logging.exception(f'Error processing alerts in the queue. {traceback.format_exc()}')

            # Check for pending notifications (deferred alerts past 10-second debounce window)
            try:
                debounce_threshold = current_time - timedelta(seconds=10)
                pending_alerts = cur.execute('''
                    SELECT * FROM active_alerts
                    WHERE last_notified_at = "0001-01-01T00:00:00Z"
                ''').fetchall()

                for alert_row in pending_alerts:
                    alert_id = alert_row[0]
                    first_seen_at = str_to_date(alert_row[1])

                    # Only send notification if alert is older than debounce threshold
                    if first_seen_at < debounce_threshold:
                        alert_dict = json.loads(alert_row[6])
                        fire_count = alert_row[5]
                        last_seen_at = alert_row[2]

                        notification_dict = alert_to_notification_dict(alert_id, alert_dict, fire_count, alert_row[1], last_seen_at)
                        notification_queue.put(notification_dict)

                        cur.execute('UPDATE active_alerts SET last_notified_at = ? WHERE alert_id = ?',
                                    (date_to_str(current_time), alert_id))
                        logging.info(f"Deferred notification sent for {alert_id} after debounce period.")

                conn.commit()
            except Exception:
                logging.exception(f'Error processing pending notifications: {traceback.format_exc()}')

            try:
                # Check if is necessary to send reminder or cleanup old alarms
                if current_time.time() >= active_alerts_reminder_time and ( last_reminder_execution_day is None or current_time.date() > last_reminder_execution_day ):
                    alerts_in_db = cur.execute('SELECT * FROM active_alerts').fetchall()
                    logging.info(f'Found {len(alerts_in_db)} active alerts in the DB.')
                    logging.debug(f'Existing alerts in DB: {alerts_in_db}')

                    for alert in alerts_in_db:
                        alert_id = alert[0]
                        last_notified_at = alert[3]
                        first_seen_at = alert[1]
                        
                        # Cleanup old alarms
                        if current_time - str_to_date(first_seen_at) > timedelta(seconds=maximum_age_of_alert_in_seconds):
                            cur.execute('''
                                    DELETE FROM active_alerts
                                    WHERE alert_id = ?
                                ''', (alert_id,))
                            continue
                        
                        # Send alarm reminders
                        if current_time - str_to_date(last_notified_at) > timedelta(seconds=minimum_age_of_active_alert_to_include_in_reminder_in_seconds):
                            
                            alert_dict = json.loads(alert[6])
                            fire_count = alert[5]
                            
                            last_seen_at = alert[2]
                            notification_dict = alert_to_notification_dict(alert_id, alert_dict, fire_count, first_seen_at, last_seen_at)
                            notification_queue.put(notification_dict)
                            logging.info(f'Sent reminder for alert id {alert_id}/{notification_dict["alert_name"]}')
                            
                            cur.execute('''
                                UPDATE active_alerts
                                SET last_notified_at = ?
                                WHERE alert_id = ?
                            ''', (date_to_str(current_time), alert_id))
                    
                    # Update alert_reminder table with the last execution time.
                    cur.execute('''
                        INSERT INTO alert_reminder (notification_id, last_executed_at)
                        VALUES (?, ?)
                        ON CONFLICT(notification_id)
                        DO UPDATE SET last_executed_at = excluded.last_executed_at
                    ''', ("last", date_to_str(current_time)))

                    conn.commit()

                    last_reminder_execution_day = current_time.date()
                    logging.info(f'Alert reminder loop successfuly executed on: {current_time}')

            except Exception as e:
                logging.error(f'Exception in the alert reminder loop: {traceback.format_exc()}')
                sleep(10)
                
    @app.get("/health")
    async def health_check():
        return JSONResponse(content={"status": "healthy"}, status_code=200)

    @app.post('/log')
    async def set_log_level(data):
        level = data.get('level', '').upper()
        valid_levels = {
            'CRITICAL': logging.CRITICAL,
            'ERROR': logging.ERROR,
            'WARNING': logging.WARNING,
            'INFO': logging.INFO,
            'DEBUG': logging.DEBUG,
        }
        if level in valid_levels:
            logging.getLogger().setLevel(valid_levels[level])
            return JSONResponse(content={'status': 'success', 'message': f'Log level set to {level}'}, status_code=fastapi_status.HTTP_200_OK)
        else:
            return JSONResponse(content={'status': 'error', 'message': 'Invalid log level'}, status_code=fastapi_status.HTTP_400_BAD_REQUEST)

    # Route to handle incoming Grafana alerts
    @app.post('/grafana-webhook')
    async def grafana_webhook(body: Request):
        # Get the incoming alert data from the request
        try:
            alert_data = await body.json()
        except:
            return JSONResponse(content={'status': 'error', 'message': 'Invalid data'}, status_code=fastapi_status.HTTP_400_BAD_REQUEST)
        logging.info(f'Received data: {alert_data}')
        if not alert_data:
            return JSONResponse(content={'status': 'error', 'message': 'Invalid data'}, status_code=fastapi_status.HTTP_400_BAD_REQUEST)
        else:
            logging.debug(f"Received alert data: {json.dumps(alert_data, indent=4)}")
        try:
            # Process each alert
            alerts = alert_data.get('alerts', [])
            for alert in alerts:
                status = alert.get('status')
                alert_labels = alert.get('labels', {})
                name = alert_labels.get('rulename', alert_labels.get('alertname', 'None'))
                # annotations = alert.get('annotations', {})
                starts_at = alert.get('startsAt')
                ends_at = alert.get('endsAt')
                
                logging.debug(f"Pushed new alert to the alert queue: {name}, status: {status}, starts_at: {starts_at}, ends_at: {ends_at}")

                # Push the alert to the queue
                alert_queue.put(json.dumps(alert))
            
            return JSONResponse(content={'status': 'success', 'message': 'Alert queued'}, status_code=fastapi_status.HTTP_200_OK)

        except Exception as e:
            logging.error(f"Error processing alert: {e}")
            return JSONResponse(content={'status': 'error', 'message': 'Error processing alert'}, status_code=fastapi_status.HTTP_400_BAD_REQUEST)


    def start_daemons():
        alert_processing_thread = Thread(target=alert_processing_daemon, daemon=True)
        alert_processing_thread.start()

        notification_processing_thread = Thread(target=notification_processing_daemon, daemon=True)
        notification_processing_thread.start()
        logging.info("Alert and notification processing daemons started.")


    def get_grafana_token():
        # attempt to get the token from secret
        try:
            config.load_incluster_config()
            v1 = client.CoreV1Api()

            # Fetch the secret
            secret = v1.read_namespaced_secret(os.environ.get("TOKEN_NAME"), os.environ.get("NAMESPACE"))

            grafana_token = base64.b64decode(secret.data["token"]).decode("utf-8")

            return grafana_token
        except Exception: # The secret doesn't exist
            logging.error(f"Failed to get the grafana token from secret: {os.environ.get('TOKEN_NAME', 'undefined')}")
            return ""

    def setup_grafana_token():
        # attempt to get the token from secret
        try:
            # create the service account
            response = requests.post(
                url=f"{grafana_service_url}/api/serviceaccounts",
                json={
                    "name": "oci-hpc-monitoring",
                    "role": "Admin"
                },
                auth=("admin", base64.b64decode(os.environ.get("GRAFANA_INITIAL_PASSWORD"))),
                timeout=30
            )

            if response.status_code != 201:
                raise Exception(f"Failed to create Grafana service account. Status code: {response.status_code}, Response: {response.text}")
            
            # create the token for the service account
            response = requests.post(
                url=f"{grafana_service_url}/api/serviceaccounts/{response.json().get("id")}/tokens",
                json={
                    "name": "oci-hpc-monitoring-token"
                },
                auth=("admin", base64.b64decode(os.environ.get("GRAFANA_INITIAL_PASSWORD"))),
                timeout=30
            )
            
            if response.status_code != 200:
                raise Exception(f"Failed to create Grafana token. Status code: {response.status_code}, Response: {response.text}")

            grafana_token = response.json().get("key")
            
            # Store the token in a Kubernetes secret
            config.load_incluster_config()
            v1 = client.CoreV1Api()

            secret = client.V1Secret(
                api_version="v1",
                kind="Secret",
                metadata=client.V1ObjectMeta(name=os.environ.get("TOKEN_NAME")),
                type="Opaque",
                data={
                    "token": base64.b64encode(grafana_token.encode("utf-8")).decode("utf-8"),
                }
            )

            api_response = v1.create_namespaced_secret(namespace=os.environ.get("NAMESPACE"), body=secret)
            return grafana_token

        except Exception: # The secret doesn't exist
            logging.error(f"Failed to setup the grafana token: {traceback.format_exc()}")
            return ""

    if __name__ == '__main__':
        logging.basicConfig(level=logging.INFO)
        
        # Setup Grafana credentials (required for URL shortening)
        logging.info(f"Attempting to get the grafana token from the secret: {os.environ.get('TOKEN_NAME')} in namespace {os.environ.get('NAMESPACE')}")
        grafana_api_key = get_grafana_token()
        if not grafana_api_key:
            logging.info(f"Setting up the Grafana Token...")
            grafana_api_key = setup_grafana_token()
        else:
            logging.info(f"Successfuly fetched the Grafana Token.")

                
        base64_encoded_grafana_api_key = os.environ.get("grafana_api_key", "")

        # Load and render the common template
        with open(template_path) as f:
            template = Template(f.read())

        start_daemons()

        uvicorn.run(app, host="0.0.0.0", port=8000, access_log=False)
    