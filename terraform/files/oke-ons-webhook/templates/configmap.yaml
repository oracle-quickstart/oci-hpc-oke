apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "oke-ons-webhook.fullname" . }}
  labels:
    {{- include "oke-ons-webhook.labels" . | nindent 4 }}
data:
  notification_template.j2: |
    {{ .Files.Get "files/notification_template.j2" | nindent 4 }}
  script.py: |
    # /// script
    # requires-python = ">=3.12"
    # dependencies = [
    #   "oci",
    #   "requests",
    #   "uvicorn",
    #   "fastapi",
    #   "kubernetes",
    #   "jinja2"
    # ]
    # ///

    import base64
    import hashlib
    import json
    import logging
    import os
    import queue
    import re
    import sqlite3
    import sys
    import traceback

    from time import sleep
    from datetime import datetime, timedelta, time
    from pathlib import Path
    from threading import Thread

    import oci
    import requests
    from urllib.parse import urlparse

    import uvicorn
    from fastapi import FastAPI, Request
    from fastapi import status as fastapi_status
    from fastapi.responses import JSONResponse
    from jinja2 import Template
    from kubernetes import client, config

    app = FastAPI()

    topic_id = os.environ.get("ONS_TOPIC_OCID")
    template_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), "notification_template.j2")
    db_dir = "/tmp"
    db_name = 'grafana_alert_processing_daemon.sqlite'

    grafana_service_url = os.environ.get("GRAFANA_SERVICE_URL")
    grafana_api_key = os.environ.get("grafana_api_key", "")

    minimum_age_of_active_alert_to_include_in_reminder_in_seconds = int(os.environ.get("MIN_AGE_OF_ACTIVE_ALERT_TO_INCLUDE_IN_REMINDER_IN_SECONDS", "43200")) #12 hours
    maximum_age_of_alert_in_seconds = int(os.environ.get("MAX_AGE_OF_ALERT_IN_SECONDS", "259200")) #3 days
    active_alerts_reminder_time = time(11, 0)
    push_to_oci_topic_interval_seconds = int(os.environ.get("PUSH_TO_OCI_TOPIC_INTERVAL_SECONDS", "30"))
    aggregate_notifications = True
    max_notification_size = 64000


    alert_queue = queue.Queue(maxsize=1000)
    notification_queue = queue.Queue(maxsize=1000)

    def base64_decode(s):
        return base64.b64decode(s).decode('utf-8')

    def utf8len(s):
        return len(s.encode('utf-8'))

    def str_to_date(text_date):
        clean_date_match = re.match(r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}", text_date)
        if clean_date_match:
            clean_date = clean_date_match.group(0)
            return datetime.strptime(f'{clean_date}Z', '%Y-%m-%dT%H:%M:%SZ')


    def date_to_str(date):
        return date.strftime('%Y-%m-%dT%H:%M:%SZ')


    def push_notifications_to_oci_topic(notification_client, notifications):
        alert_message = "\n".join([template.render(**notification) for notification in notifications])
        
        message_details = oci.ons.models.MessageDetails(
            title="GPU Cluster Alert",
            body=alert_message
        )
        
        response = notification_client.publish_message(
            topic_id=topic_id,  # Use the dynamically fetched topic_id
            message_details=message_details,
            retry_strategy=oci.retry.DEFAULT_RETRY_STRATEGY
        )

        logging.info(f"Message published. Message ID: {response.data.message_id}")


    def shorten_url(grafana_service_url, grafana_api_key, url):
        # Shorten URL using Grafana API

        parsed_url = urlparse(url)
        
        path = parsed_url.geturl().replace(f'{parsed_url.scheme}://{parsed_url.netloc}/', "")
        headers = {
            "Accept": "application/json",
            "Content-Type": "application/json",
            "Authorization": f"Bearer {grafana_api_key}"
        }

        payload = {
            "path": path
        }

        response = requests.post(f"{grafana_service_url.rstrip('/')}/api/short-urls", json=payload, headers=headers)
        logging.debug(f'Attempt to shorten {path} returned code {response.status_code} and response: {response.json()}')
        
        if response.status_code == 200:
            short_url = response.json().get("url") 
            parsed_short_url = urlparse(short_url)
            netloc = parsed_url.netloc.split(":")[0]
            ip_only = False
            match = re.match(r"^(\d{1,3}\.?){4}$", netloc)
            if match:
                ip_only = True
            if not ip_only:
                scheme = "https"
                return parsed_short_url.geturl().replace(f'{parsed_short_url.scheme}://{parsed_url.netloc}/', f"{scheme}://{netloc}/")
            return short_url
        else:
            return url


    def alert_to_notification_dict(alert_id, alert_dict, fire_count, first_seen_at, last_seen_at):
        annotations = alert_dict.get("annotations", {})
        labels = alert_dict.get("labels", {})
        silence_url = annotations.get("silence_url", alert_dict.get("silenceURL", "N/A")).strip()
        panel_url =  annotations.get("panel_url", alert_dict.get("panelURL", "N/A")).strip()
        if silence_url and silence_url != "N/A":
            shorten_silence_url = shorten_url(grafana_service_url, grafana_api_key, silence_url)
        else:
            shorten_silence_url = "N/A"
        if panel_url and panel_url != "N/A":
            shorten_panel_url = shorten_url(grafana_service_url, grafana_api_key, panel_url)
        else:
            shorten_panel_url = "N/A"
        return {
            "alert_id": alert_id,
            "alert_name": labels.get("rulename", labels.get("alertname", "N/A")),
            "alert_status": alert_dict.get("status", "N/A"),
            "starts_at": alert_dict.get("startsAt", "N/A"),
            "ends_at": alert_dict.get("endsAt", "N/A") if alert_dict.get("endsAt", "N/A") != '0001-01-01T00:00:00Z' else "-",
            "description": annotations.get("summary", "N/A"),
            "hostname": labels.get("hostname", labels.get("Hostname", "N/A")),
            "node": labels.get("node", "N/A"),
            "oci_name": labels.get("oci_name", "N/A"),
            "serial_number": labels.get("host_serial_number", labels.get("serial", "N/A")),
            "interface": labels.get("interface", "N/A"),
            "rdma_device": labels.get("rdma_device", "N/A"),
            "gpu": labels.get("gpu", "N/A"),
            "fire_count": fire_count,
            "first_seen_at": first_seen_at,
            "last_seen_at": last_seen_at,
            "silence_url": shorten_silence_url,
            "panel_url": shorten_panel_url 
        }


    def notification_processing_daemon():
        try:
            # Use Instance Principals for authentication
            signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
            notification_client = oci.ons.NotificationDataPlaneClient({}, signer=signer)
        except Exception as e:
            logging.error(f"Error initializing OCI client: {e}")
            sys.exit(1)

        notifications = []

        while True:

            if aggregate_notifications:
                # Get all the notifications from the queue
                try:
                    notification_from_queue = notification_queue.get(block=True, timeout=push_to_oci_topic_interval_seconds)
                    
                    # Check if the aggregated notification text is larger than max_notification_size (ONS supports maximum 64 KB)
                    if utf8len("\n".join([template.render(**notification) for notification in (notifications + [notification_from_queue])])) > max_notification_size:
                        push_notifications_to_oci_topic(notification_client, notifications)
                        notifications = [notification_from_queue]
                    else:
                        notifications.append(notification_from_queue)
                        # wait three seconds before checking it the queue is empty (avoid accumulating too much delay sending notifications when aggregate_notifications is True
                        sleep(3)
                      
                        if notification_queue.empty():
                            push_notifications_to_oci_topic(notification_client, notifications)
                            notifications = []
                
                # When queue is empty and there are notifications to be sent, send them
                except queue.Empty:
                    if notifications:
                        push_notifications_to_oci_topic(notification_client, notifications)
                    notifications = []
            else:
                try:
                    notifications.append(notification_queue.get(block=True, timeout=push_to_oci_topic_interval_seconds))
                except queue.Empty:
                    pass
                
                if notifications:
                    push_notifications_to_oci_topic(notification_client, notifications)
                notifications = []


    def alert_processing_daemon():
        try:
            os.makedirs(db_dir, exist_ok=True)
            conn = sqlite3.connect(os.path.join(db_dir, db_name))
            cur = conn.cursor()
            # Create active_alerts table if it doesn't exist
            cur.execute('''CREATE TABLE IF NOT EXISTS active_alerts (
                alert_id TEXT PRIMARY KEY,
                first_seen_at TEXT NOT NULL,
                last_seen_at TEXT NOT NULL,
                last_notified_at TEXT NOT NULL,
                ended_at TEXT NOT NULL,
                fire_count INTEGER NOT NULL,
                payload TEXT NOT NULL
            )''')
            conn.commit()
            # Create alert_reminder table if it doesn't exist
            cur.execute('''CREATE TABLE IF NOT EXISTS alert_reminder (
                notification_id TEXT PRIMARY KEY,
                last_executed_at TEXT NOT NULL
            )''')
            conn.commit()
        except Exception as e:
            logging.error(f'Failed to create the databases for alerts {e}. Exiting..')
            sys.exit(1)

        last_reminder_execution_date_from_db = cur.execute('SELECT * FROM alert_reminder WHERE notification_id = ?', ("last",)).fetchone()
        if last_reminder_execution_date_from_db:
            last_reminder_execution_day = str_to_date(last_reminder_execution_date_from_db[1]).date()
        else:
            last_reminder_execution_day = None
        
        logging.info(f'Last reminder execution day: {last_reminder_execution_day}')
        
        while True:
            # Process alerts in the queue
            current_time = datetime.now()

            try:
                alert = alert_queue.get(block=True, timeout=30)

                # Parse the alert JSON
                alert_dict = json.loads(alert)
                
                # Skip alert with name "DatasourceNoData"
                alert_labels = alert_dict.get('labels', {})
                alert_name = alert_labels.get('rulename', alert_labels.get('alertname', 'None'))
                

                if alert_name and alert_name != "DatasourceNoData":

                    # Add alert details into the database
                    
                    starts_at = str_to_date(alert_dict.get('startsAt'))
                    # 
                    fingerprint = alert_dict.get("fingerprint", "")
                    if fingerprint:
                        alert_id = hashlib.sha256(f'{fingerprint}'.encode('utf-8')).hexdigest()
                    else:
                        alert_id = hashlib.sha256(f'{alert_name}-{alert_dict.get("startsAt")}'.encode('utf-8')).hexdigest()
                    ends_at = str_to_date(alert_dict.get('endsAt'))
                    current_status = alert_dict.get('status', "Status Unknown")

                    # Check if the alert is present in the database
                    alert_in_db = cur.execute('SELECT * FROM active_alerts WHERE alert_id = ?', (alert_id,)).fetchone()
                    if alert_in_db:
                        # If the alert is resolved, clear the alert from database and send load the notification into the queue
                        if ends_at > starts_at and current_status != "firing":
                            notification_dict = alert_to_notification_dict(alert_in_db[0], alert_dict, alert_in_db[5], alert_in_db[1], date_to_str(ends_at))
                            notification_queue.put(notification_dict)
                            
                            logging.info(f"Alert {alert_id}/{alert_name} resolved. Notification sent.")
                            
                            cur.execute('''
                                DELETE FROM active_alerts
                                WHERE alert_id = ?
                            ''', (alert_id,))
                        else: 
                            # If the alert is not resolved, increment the fire_count
                            cur.execute('''
                                UPDATE active_alerts
                                SET last_seen_at = ?, fire_count = fire_count + 1, payload = ?
                                WHERE alert_id = ?
                            ''', (date_to_str(current_time), alert, alert_id))
                            logging.info(f"Alert {alert_id}/{alert_name} still active. Incremented Fire Count.")
                    else:
                        # If alert received for the first time and is already clear, just send notification
                        if ends_at > starts_at and current_status != "firing":
                            notification_dict = alert_to_notification_dict(alert_id, alert_dict, 1, alert_dict.get('startsAt'), alert_dict.get('endsAt'))
                            logging.info(f"New Alert {alert_id}/{alert_name} seen for the first time with status resolved.")

                        # otherwise, load the alert into the database and send notification
                        else:
                            notification_dict = alert_to_notification_dict(alert_id, alert_dict, 1, alert_dict.get('startsAt'), alert_dict.get('endsAt'))
                            cur.execute('''
                                INSERT INTO active_alerts (alert_id, first_seen_at, last_seen_at, last_notified_at, ended_at, fire_count, payload)
                                VALUES (?, ?, ?, ?, ?, ?, ?)
                            ''', (alert_id, date_to_str(current_time), date_to_str(current_time), date_to_str(current_time), "0001-01-01T00:00:00Z", 1, alert))
                            logging.info(f"New Alert {alert_id}/{alert_name} seen for the first. Added to the database.")
                        notification_queue.put(notification_dict)

                    # Commit the changes to the database
                    conn.commit()
                else:
                    logging.debug(f'Skipping processing alert with name DatasourceNoData or empty: {alert}')
                    pass
            except queue.Empty:
                pass

            # Check if is necessary to send reminder or cleanup old alarms
            if current_time.time() >= active_alerts_reminder_time and ( last_reminder_execution_day is None or current_time.date() > last_reminder_execution_day ):
                alerts_in_db = cur.execute('SELECT * FROM active_alerts').fetchall()
                logging.info(f'Found {len(alerts_in_db)} active alerts in the DB.')
                logging.debug(f'Existing alerts in DB: {alerts_in_db}')

                for alert in alerts_in_db:
                    alert_id = alert[0]
                    last_notified_at = alert[3]
                    first_seen_at = alert[1]
                    
                    # Cleanup old alarms
                    if current_time - str_to_date(first_seen_at) > timedelta(seconds=maximum_age_of_alert_in_seconds):
                        cur.execute('''
                                DELETE FROM active_alerts
                                WHERE alert_id = ?
                            ''', (alert_id,))
                        continue
                    
                    # Send alarm reminders
                    if current_time - str_to_date(last_notified_at) > timedelta(seconds=minimum_age_of_active_alert_to_include_in_reminder_in_seconds):
                        
                        alert_dict = json.loads(alert[6])
                        fire_count = alert[5]
                        
                        last_seen_at = alert[2]
                        notification_dict = alert_to_notification_dict(alert_id, alert_dict, fire_count, first_seen_at, last_seen_at)
                        notification_queue.put(notification_dict)
                        logging.info(f'Sent reminder for alert id {alert_id}/{notification_dict["alert_name"]}')
                        
                        cur.execute('''
                            UPDATE active_alerts
                            SET last_notified_at = ?
                            WHERE alert_id = ?
                        ''', (date_to_str(current_time), alert_id))
                
                # Update alert_reminder table with the last execution time.
                cur.execute('''
                    INSERT INTO alert_reminder (notification_id, last_executed_at)
                    VALUES (?, ?)
                    ON CONFLICT(notification_id)
                    DO UPDATE SET last_executed_at = excluded.last_executed_at
                ''', ("last", date_to_str(current_time)))

                conn.commit()

                last_reminder_execution_day = current_time.date()
                logging.info(f'Alert reminder loop successfuly executed on: {current_time}')

    @app.get("/health")
    async def health_check():
        return JSONResponse(content={"status": "healthy"}, status_code=200)

    @app.post('/log')
    async def set_log_level(data):
        level = data.get('level', '').upper()
        valid_levels = {
            'CRITICAL': logging.CRITICAL,
            'ERROR': logging.ERROR,
            'WARNING': logging.WARNING,
            'INFO': logging.INFO,
            'DEBUG': logging.DEBUG,
        }
        if level in valid_levels:
            logging.getLogger().setLevel(valid_levels[level])
            return JSONResponse(content={'status': 'success', 'message': f'Log level set to {level}'}, status_code=fastapi_status.HTTP_200_OK)
        else:
            return JSONResponse(content={'status': 'error', 'message': 'Invalid log level'}, status_code=fastapi_status.HTTP_400_BAD_REQUEST)

    # Route to handle incoming Grafana alerts
    @app.post('/grafana-webhook')
    async def grafana_webhook(body: Request):
        # Get the incoming alert data from the request
        try:
            alert_data = await body.json()
        except:
            return JSONResponse(content={'status': 'error', 'message': 'Invalid data'}, status_code=fastapi_status.HTTP_400_BAD_REQUEST)
        logging.info(f'Received data: {alert_data}')
        if not alert_data:
            return JSONResponse(content={'status': 'error', 'message': 'Invalid data'}, status_code=fastapi_status.HTTP_400_BAD_REQUEST)
        else:
            logging.debug(f"Received alert data: {json.dumps(alert_data, indent=4)}")
        try:
            # Process each alert
            alerts = alert_data.get('alerts', [])
            for alert in alerts:
                status = alert.get('status')
                alert_labels = alert.get('labels', {})
                name = alert_labels.get('rulename', alert_labels.get('alertname', 'None'))
                # annotations = alert.get('annotations', {})
                starts_at = alert.get('startsAt')
                ends_at = alert.get('endsAt')
                
                logging.debug(f"Pushed new alert to the alert queue: {name}, status: {status}, starts_at: {starts_at}, ends_at: {ends_at}")

                # Push the alert to the queue
                alert_queue.put(json.dumps(alert))
            
            return JSONResponse(content={'status': 'success', 'message': 'Alert queued'}, status_code=fastapi_status.HTTP_200_OK)

        except Exception as e:
            logging.error(f"Error processing alert: {e}")
            return JSONResponse(content={'status': 'error', 'message': 'Error processing alert'}, status_code=fastapi_status.HTTP_400_BAD_REQUEST)


    def start_daemons():
        alert_processing_thread = Thread(target=alert_processing_daemon, daemon=True)
        alert_processing_thread.start()

        notification_processing_thread = Thread(target=notification_processing_daemon, daemon=True)
        notification_processing_thread.start()
        logging.info("Alert and notification processing daemons started.")


    def get_grafana_token():
      # attempt to get the token from secret
        try:
            config.load_incluster_config()
            v1 = client.CoreV1Api()

            # Fetch the secret
            secret = v1.read_namespaced_secret(os.environ.get("TOKEN_NAME"), os.environ.get("NAMESPACE"))

            grafana_token = base64.b64decode(secret.data["token"]).decode("utf-8")

            return grafana_token
        except Exception: # The secret doesn't exist
            logging.error(f"Failed to get the grafana token from secret: {os.environ.get('TOKEN_NAME', 'undefined')}")
            return ""

    def setup_grafana_token():
      # attempt to get the token from secret
        try:
            # create the service account
            response = requests.post(
              url=f"{grafana_service_url}/api/serviceaccounts",
              json={
                  "name": "oci-hpc-monitoring",
                  "role": "Admin"
              },
              auth=("admin", base64.b64decode(os.environ.get("GRAFANA_INITIAL_PASSWORD"))),
              timeout=30
            )

            if response.status_code != 201:
                raise Exception(f"Failed to create Grafana service account. Status code: {response.status_code}, Response: {response.text}")
            
            # create the token for the service account
            response = requests.post(
                url=f"{grafana_service_url}/api/serviceaccounts/{response.json().get("id")}/tokens",
                json={
                    "name": "oci-hpc-monitoring-token"
                },
                auth=("admin", base64.b64decode(os.environ.get("GRAFANA_INITIAL_PASSWORD"))),
                timeout=30
            )
            
            if response.status_code != 200:
                raise Exception(f"Failed to create Grafana token. Status code: {response.status_code}, Response: {response.text}")

            grafana_token = response.json().get("key")
            
            # Store the token in a Kubernetes secret
            config.load_incluster_config()
            v1 = client.CoreV1Api()

            secret = client.V1Secret(
                api_version="v1",
                kind="Secret",
                metadata=client.V1ObjectMeta(name=os.environ.get("TOKEN_NAME")),
                type="Opaque",
                data={
                    "token": base64.b64encode(grafana_token.encode("utf-8")).decode("utf-8"),
                }
            )

            api_response = v1.create_namespaced_secret(namespace=os.environ.get("NAMESPACE"), body=secret)
            return grafana_token

        except Exception: # The secret doesn't exist
            logging.error(f"Failed to setup the grafana token: {traceback.format_exc()}")
            return ""

    if __name__ == '__main__':
        logging.basicConfig(level=logging.INFO)
        
        # Setup Grafana credentials (required for URL shortening)
        logging.info(f"Attempting to get the grafana token from the secret: {os.environ.get('TOKEN_NAME')} in namespace {os.environ.get('NAMESPACE')}")
        grafana_api_key = get_grafana_token()
        if not grafana_api_key:
            logging.info(f"Setting up the Grafana Token...")
            grafana_api_key = setup_grafana_token()
        else:
            logging.info(f"Successfuly fetched the Grafana Token.")

                
        base64_encoded_grafana_api_key = os.environ.get("grafana_api_key", "")

        # Load and render the common template
        with open(template_path) as f:
            template = Template(f.read())

        start_daemons()

        uvicorn.run(app, host="0.0.0.0", port=8000, access_log=False)
    